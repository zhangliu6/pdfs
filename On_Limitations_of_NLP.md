Much of the current NLP research seems to over-simplify language.
Of course, it is understandable because we want to use computational and mathematical symbols to represent natural language so that it is able to be processed - ‘understandable’ - by the machines.
However, one fundamental problem with this approach is that the complexity of the interactions between language and speakers, contexts are overlooked.
This would have far-reaching implications, especially in NLP applications such as dialogue systems.

Language is meaningless without CONTEXT; it does not happen in a vacuum.
However, most of the NLP applications treat language as something standing on its own.
Granted the high accuracy of advanced parsing techniques and advanced classification methods such as HMM.
What we seem to neglect all along is the context where all these linguistic data are being produced.
For example, the world’s most advanced parser will fail to analyse the complex meaning of the seemingly simple utterance ‘I am good’.
The speaker might be replying casually to a greeting.
The speaker might have a bad day and do not want to be disturbed.

In short, NLP, especially higher level NLP, has to take into consideration not only the quantitative, but more importantly, the qualitative aspect of things.
Yes, but we cannot and should not simply throw linguistic data to the machine and treat any linguistic data indiscriminately.
In order to achieve real NLP, we have to incorporate the current research in linguistic and language.
And that requires collaboration between linguists and computer scientists to work together.
That requires us to have a brand new mindset towards NLP.

Natural **discourse** processing 
